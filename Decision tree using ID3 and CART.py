# -*- coding: utf-8 -*-
"""decision tree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GEbMHzWzKlgPN7ydAbzZ_92e8lgBySlY
"""

import pandas as pd
import numpy as np
import pprint
from collections import defaultdict
from sklearn.model_selection import train_test_split

class ID3DecisionTree:
    """
    Decision tree using ID3
    """
    def __init__(self):
        pass

    def get_entropy(self, df):
        # TODO: Calculate the entropy(s)
        entropy = 0
        target_class = df.keys()[-1]
        values = df[target_class].unique()
        for value in values:
            prob = df[target_class].value_counts()[value] / len(df[target_class])
            entropy += -prob * np.log2(prob)
        return entropy

    def split_table(self, df, attr, value):
        return df[df[attr] == value].reset_index(drop=True)

    def fit(self, df):
        # compute entropy(s)
        entropy_s = self.get_entropy(df)

        # List of all attrs except the class
        attrs = df.keys()[:-1]

        tree = defaultdict(dict)

        max_gain = 0
        max_gain_attr = None
        # for all attr, find the attr with max gain
        for attr in attrs:
            values = df[attr].unique()
            # TODO: Calculate the entropies of unique values of attr
            # Example: attr=Outlook has entropies = [0.971, 0.971, 0]
            entropies = []
            # TODO: Calculate the probabilities of unique values of attr
            # Example: attr=Outlook has entropies = [5/14, 5/14, 0]
            probabilities = []

            for value in values:
                subset = df[df[attr] == value]
                subset_entropy = self.get_entropy(subset)
                entropies.append(subset_entropy)
                probabilities.append(len(subset) / len(df))

            # Calculate the average information entropy
            average_info_entropy = 0
            for probability, entropy in zip(probabilities, entropies):
                average_info_entropy += probability * entropy
                # TODO: Calculate attr gain
                attr_gain = 0
                attr_gain = entropy_s - average_info_entropy

                # TODO: Update the max_gain_attr
                if attr_gain > max_gain:
                    max_gain = attr_gain
                    max_gain_attr = attr

        if max_gain_attr is None:
            target_class = df.keys()[-1]
            return df[target_class].value_counts().idxmax()

        # Split the df based on the values of max_gain_attr
        values = df[max_gain_attr].unique()
        for value in values:
            new_df = self.split_table(df, max_gain_attr, value)
            class_values, class_counts = np.unique(new_df[new_df.keys()[-1]], return_counts=True)

            # If it is a pure class, then this is the leaf node
            # else divide the new_df further
            if len(class_counts) == 1:
                tree[max_gain_attr][value] = class_values[0]
            else:
                tree[max_gain_attr][value] = self.fit(new_df)

        return tree

    def predict(self, example, tree, default=None):
        attribute = next(iter(tree))
        if example[attribute] in tree[attribute].keys():
            subtree = tree[attribute][example[attribute]]
            if isinstance(subtree, dict):
                return self.predict(example, subtree)
            else:
                return subtree
        else:
            return default

    def evaluate(self, tree, df):
        # TODO: Complete the evaluate method
        correct = 0
        for _, row in df.iterrows():
            prediction = self.predict(row, tree)
            if prediction == row[-1]:
                correct += 1
        accuracy = (correct / len(df))*100
        return accuracy

class CARTDecisionTree:
    def __init__(self):
        self.tree = None

    def fit(self, df):
        self.tree = self.build_tree(df, 'play')
        return self.tree

    def gini_impurity(self, labels):
        if len(labels) == 0:
            return 0.0

        class_counts = labels.value_counts()
        total_samples = len(labels)
        gini_impurity = 1.0
        for count in class_counts:
            proportion = count / total_samples
            gini_impurity -= proportion ** 2

        return gini_impurity

    def find_best_split(self, df, target_column):
        best_split_attr = None
        best_split_value = None
        min_gini = float('inf')

        for attribute in df.columns:
            if attribute == target_column:
                continue

            values = df[attribute].unique()

            for value in values:
                left_subset = df[df[attribute] <= value]
                right_subset = df[df[attribute] > value]

                left_impurity = self.gini_impurity(left_subset[target_column])
                right_impurity = self.gini_impurity(right_subset[target_column])

                weighted_impurity = (len(left_subset) / len(df)) * left_impurity \
                                    + (len(right_subset) / len(df)) * right_impurity

                if weighted_impurity < min_gini:
                    min_gini = weighted_impurity
                    best_split_attr = attribute
                    best_split_value = value

        return best_split_attr, best_split_value

    def build_tree(self, df, target_column):
        if len(df[target_column].unique()) == 1:
            return {'class': df[target_column].iloc[0]}

        best_split_attr, best_split_value = self.find_best_split(df, target_column)

        if best_split_attr is None:
            return {'class': df[target_column].mode().iloc[0]}

        left_subset = df[df[best_split_attr] <= best_split_value]
        right_subset = df[df[best_split_attr] > best_split_value]

        left_subtree = self.build_tree(left_subset, target_column)
        right_subtree = self.build_tree(right_subset, target_column)

        return {'attribute': best_split_attr,
                  'value': best_split_value,
                  'left': left_subtree,
                  'right': right_subtree}


    def predict(self, example, tree):
        if 'class' in tree:
            return tree['class']
        else:
            attr_value = example[tree['attribute']]
            if attr_value <= tree['value']:
                return self.predict(example, tree['left'])
            else:
                return self.predict(example, tree['right'])

    def evaluate(self, tree, df):
          correct = 0
          for _, row in df.iterrows():
              prediction = self.predict(row, tree)
              if prediction == row[-1]:
                  correct += 1
          accuracy = (correct / len(df))*100
          return accuracy

data_path = '/content/play_tennis.csv'
df = pd.read_csv(data_path)
df = df.drop('day', axis=1)
X = df.drop('play', axis=1)
y = df['play']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

train_df = X_train.join(y_train)
test_df = X_test.join(y_test)

new_day_info = {'day':'D15','outlook': 'Rain', 'temp': 'Cool', 'humidity': 'Normal', 'wind': 'Weak'}
print("A new day's information:",new_day_info,"\n")

# Train the ID3 model
model = ID3DecisionTree()
tree = model.fit(train_df)

print("For ID3, the tree:\n")
pprint.pprint(tree)

prediction = model.predict(new_day_info, tree)
print("For ID3: Will he play tennis? Answer:", prediction)

acc = model.evaluate(tree, test_df)
print("After testing the test dataset\n","For ID3: Accuracy: {:.2f}%".format(acc))

# Train the CART model
model = CARTDecisionTree()
tree = model.fit(train_df)

print("For CART, the tree:\n")
pprint.pprint(tree)

prediction = model.predict(new_day_info, tree)
print("\nFor CART: Will he play tennis? Answer:", prediction)

acc = model.evaluate(tree, test_df, target_column='play')
print("After testing the test dataset\n","For CART: Accuracy: {:.2f}%".format(acc))